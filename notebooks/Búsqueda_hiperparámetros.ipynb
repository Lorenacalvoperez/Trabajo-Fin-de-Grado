{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d1dbac2-6add-42f6-b4ef-d7285eda4c7f",
   "metadata": {},
   "source": [
    "# Carga de csv necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35cb2922-9324-488e-9cd7-b4031a691b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def cargar_csv(nombre_archivo):\n",
    "    \"\"\"\n",
    "    Carga un archivo CSV desde la ruta 'Resultados/Archivos_csv/Datos' en un DataFrame de pandas.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    nombre_archivo : str\n",
    "        Nombre del archivo CSV a cargar.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame con los datos cargados del archivo CSV.\n",
    "    \"\"\"\n",
    "    # Construir la ruta completa\n",
    "    ruta_archivo = os.path.join(\"..\",\"Resultados\", \"Archivos_csv\", \"Datos\", nombre_archivo)\n",
    "    \n",
    "    # Leer el CSV\n",
    "    df = pd.read_csv(ruta_archivo, encoding=\"utf-8\")\n",
    "    print(f\"Archivo '{ruta_archivo}' cargado correctamente.\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ab47cb2-6d7a-4324-b2ff-4cc300599872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo '..\\Resultados\\Archivos_csv\\Datos\\Datos_Parkinson.csv' cargado correctamente.\n",
      "Archivo '..\\Resultados\\Archivos_csv\\Datos\\Datos_contaminación_aire.csv' cargado correctamente.\n",
      "Archivo '..\\Resultados\\Archivos_csv\\Datos\\Datos_muertes_agua.csv' cargado correctamente.\n",
      "Archivo '..\\Resultados\\Archivos_csv\\Datos\\Datos_exp_plomo.csv' cargado correctamente.\n",
      "Archivo '..\\Resultados\\Archivos_csv\\Datos\\Datos_uso_pepticidas.csv' cargado correctamente.\n",
      "Archivo '..\\Resultados\\Archivos_csv\\Datos\\Datos_precipitaciones.csv' cargado correctamente.\n"
     ]
    }
   ],
   "source": [
    "df_parkinson = cargar_csv(\"Datos_Parkinson.csv\")\n",
    "df_contaminacion = cargar_csv(\"Datos_contaminación_aire.csv\")  # Tasa de contaminación\n",
    "df_calidad_agua = cargar_csv(\"Datos_muertes_agua.csv\")\n",
    "df_plomo = cargar_csv(\"Datos_exp_plomo.csv\")  # Tasa de carga de enfermedad por exposición al plomo\n",
    "df_pesticidas = cargar_csv(\"Datos_uso_pepticidas.csv\")  # Uso de pesticidas en toneladas\n",
    "df_precipitaciones = cargar_csv(\"Datos_precipitaciones.csv\")  # Precipitaciones en mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a4725-7fce-47ec-840c-863d900c3d20",
   "metadata": {},
   "source": [
    "# Selección de Modelo GLM más apropiado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431ee7e-e64a-472a-b592-4bfc4426ce43",
   "metadata": {},
   "source": [
    "## Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cb258d1-705c-4841-a46c-63a09b04128d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'Tabla_modelos.csv' cargado correctamente.\n",
      "Media de Parkinson: 81.1569328286849\n",
      "Varianza de Parkinson: 6353.157591496592\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "df = cargar_csv(\"Tabla_modelos.csv\")\n",
    "\n",
    "# Calcular la media y la varianza de la variable dependiente 'Parkinson'\n",
    "media = df['Parkinson'].mean()\n",
    "varianza = df['Parkinson'].var()\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Media de Parkinson: {media}\")\n",
    "print(f\"Varianza de Parkinson: {varianza}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f68c9-fd7c-4ffc-89e0-06a44f90aabd",
   "metadata": {},
   "source": [
    "## Cuassi-poisson "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "808ee679-c93d-4eec-9a9c-1b3cf693275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_quasi_poisson(df,variables_independientes, variable_dependiente, test_size=0.2, scaler=False):\n",
    "    import statsmodels.api as sm\n",
    "    from patsy import dmatrices\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "    # Construir fórmula en función de variables\n",
    "    transformaciones = {\n",
    "        'Contaminacion_aire': ['Contaminacion_aire', 'I(Contaminacion_aire**2)'],\n",
    "        'Muertes_agua': ['Muertes_agua', 'I(Muertes_agua**2)','I(Muertes_agua**3)'],\n",
    "        'Exp_plomo': ['Exp_plomo', 'I(Exp_plomo**2)', 'I(Exp_plomo**3)'],\n",
    "        'Pesticidas': ['Pesticidas','I(Pesticidas**2)', 'I(Pesticidas**3)'],\n",
    "        'Precipitaciones': ['Precipitaciones','I(Precipitaciones**2)', 'I(Precipitaciones**3)'],\n",
    "    }\n",
    "\n",
    "    # Construir partes de fórmula\n",
    "    partes_formula = []\n",
    "    for var in variables_independientes:\n",
    "        partes_formula.extend(transformaciones.get(var, [var]))\n",
    "\n",
    "    formula = f\"{variable_dependiente} ~ \" + \" + \".join(partes_formula)\n",
    "\n",
    "    # Dividir en train/test\n",
    "    df_train, df_test = train_test_split(df, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Crear matrices con patsy\n",
    "    y_train, X_train = dmatrices(formula, data=df_train, return_type='dataframe')\n",
    "    y_test, X_test = dmatrices(formula, data=df_test, return_type='dataframe')\n",
    "\n",
    "\n",
    "    # Reset índices para alinear endog y exog\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "    if scaler:\n",
    "        scaler_model = StandardScaler()\n",
    "        intercept_train = X_train[['Intercept']]\n",
    "        intercept_test = X_test[['Intercept']]\n",
    "\n",
    "        X_train_scaled = scaler_model.fit_transform(X_train.drop(columns='Intercept'))\n",
    "        X_test_scaled = scaler_model.transform(X_test.drop(columns='Intercept'))\n",
    "\n",
    "        X_train = pd.concat([intercept_train,\n",
    "                             pd.DataFrame(X_train_scaled, columns=X_train.columns.drop('Intercept'))], axis=1)\n",
    "        X_test = pd.concat([intercept_test,\n",
    "                            pd.DataFrame(X_test_scaled, columns=X_test.columns.drop('Intercept'))], axis=1)\n",
    "\n",
    "    modelo = sm.GLM(y_train, X_train, family=sm.families.Poisson()).fit(cov_type='HC0')\n",
    "\n",
    "    print(modelo.summary())\n",
    "\n",
    "    y_pred = modelo.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\nRMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "    return modelo, rmse, mae\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b92d5c8-f3b1-4236-aca0-686505ff69ec",
   "metadata": {},
   "source": [
    "## Binomial negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "264db1a5-5929-4fb9-b82d-86bc7eb13e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_binomial_negativo(df,variables_independientes, variable_dependiente,  test_size=0.2, scaler=False):\n",
    "    import statsmodels.api as sm\n",
    "    from patsy import dmatrices\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    import numpy as np\n",
    "\n",
    "    # Construir fórmula en función de variables\n",
    "    transformaciones = {\n",
    "        'Contaminacion_aire': ['Contaminacion_aire', 'I(Contaminacion_aire**2)'],\n",
    "        'Muertes_agua': ['Muertes_agua', 'I(Muertes_agua**2)','I(Muertes_agua**3)'],\n",
    "        'Exp_plomo': ['Exp_plomo', 'I(Exp_plomo**2)', 'I(Exp_plomo**3)'],\n",
    "        'Pesticidas': ['Pesticidas','I(Pesticidas**2)', 'I(Pesticidas**3)'],\n",
    "        'Precipitaciones': ['Precipitaciones','I(Precipitaciones**2)', 'I(Precipitaciones**3)'],\n",
    "    }\n",
    "\n",
    "    # Construir partes de fórmula\n",
    "    partes_formula = []\n",
    "    for var in variables_independientes:\n",
    "        partes_formula.extend(transformaciones.get(var, [var]))\n",
    "\n",
    "    formula = f\"{variable_dependiente} ~ \" + \" + \".join(partes_formula)\n",
    "\n",
    "    # Dividir en train/test\n",
    "    df_train, df_test = train_test_split(df, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Crear matrices con patsy\n",
    "    y_train, X_train = dmatrices(formula, data=df_train, return_type='dataframe')\n",
    "    y_test, X_test = dmatrices(formula, data=df_test, return_type='dataframe')\n",
    "\n",
    "    # Resetear índices para alinear correctamente\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "    if scaler:\n",
    "        scaler_model = StandardScaler()\n",
    "        intercept_train = X_train[['Intercept']]\n",
    "        intercept_test = X_test[['Intercept']]\n",
    "\n",
    "        X_train_scaled = scaler_model.fit_transform(X_train.drop(columns='Intercept'))\n",
    "        X_test_scaled = scaler_model.transform(X_test.drop(columns='Intercept'))\n",
    "\n",
    "        X_train = pd.concat([intercept_train,\n",
    "                             pd.DataFrame(X_train_scaled, columns=X_train.columns.drop('Intercept'))], axis=1)\n",
    "        X_test = pd.concat([intercept_test,\n",
    "                            pd.DataFrame(X_test_scaled, columns=X_test.columns.drop('Intercept'))], axis=1)\n",
    "\n",
    "    modelo = sm.GLM(y_train, X_train, family=sm.families.NegativeBinomial()).fit()\n",
    "\n",
    "    print(modelo.summary())\n",
    "\n",
    "    y_pred = modelo.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\nRMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "    return modelo, rmse, mae\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd3b9bd-d5f7-45ad-a4a8-bc797f993879",
   "metadata": {},
   "source": [
    "## Cuassi-poisson vs Binomial Negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdcf0de3-0d93-43f6-9b4e-beef1a4128e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'Tabla_modelos.csv' cargado correctamente.\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:              Parkinson   No. Observations:                 4331\n",
      "Model:                            GLM   Df Residuals:                     4316\n",
      "Model Family:                 Poisson   Df Model:                           14\n",
      "Link Function:                    Log   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -48900.\n",
      "Date:                Thu, 12 Jun 2025   Deviance:                       72561.\n",
      "Time:                        11:31:06   Pearson chi2:                 8.19e+04\n",
      "No. Iterations:                     6   Pseudo R-squ. (CS):              1.000\n",
      "Covariance Type:                  HC0                                         \n",
      "==============================================================================================\n",
      "                                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "Intercept                      4.0914      0.008    483.905      0.000       4.075       4.108\n",
      "Contaminacion_aire             0.1454      0.025      5.920      0.000       0.097       0.194\n",
      "I(Contaminacion_aire ** 2)    -0.0127      0.020     -0.634      0.526      -0.052       0.026\n",
      "Muertes_agua                  -0.9674      0.077    -12.625      0.000      -1.118      -0.817\n",
      "I(Muertes_agua ** 2)           1.9930      0.164     12.153      0.000       1.672       2.314\n",
      "I(Muertes_agua ** 3)          -1.1590      0.110    -10.525      0.000      -1.375      -0.943\n",
      "Exp_plomo                     -1.6057      0.048    -33.509      0.000      -1.700      -1.512\n",
      "I(Exp_plomo ** 2)              1.5215      0.098     15.570      0.000       1.330       1.713\n",
      "I(Exp_plomo ** 3)             -0.4584      0.058     -7.922      0.000      -0.572      -0.345\n",
      "Pesticidas                     0.3174      0.027     11.969      0.000       0.265       0.369\n",
      "I(Pesticidas ** 2)            -0.5063      0.050    -10.110      0.000      -0.604      -0.408\n",
      "I(Pesticidas ** 3)             0.2375      0.031      7.701      0.000       0.177       0.298\n",
      "Precipitaciones                0.8615      0.059     14.543      0.000       0.745       0.978\n",
      "I(Precipitaciones ** 2)       -2.2044      0.140    -15.713      0.000      -2.479      -1.929\n",
      "I(Precipitaciones ** 3)        1.2710      0.092     13.773      0.000       1.090       1.452\n",
      "==============================================================================================\n",
      "\n",
      "RMSE: 43.7305\n",
      "MAE: 29.4331\n",
      "Archivo 'Tabla_modelos.csv' cargado correctamente.\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:              Parkinson   No. Observations:                 4331\n",
      "Model:                            GLM   Df Residuals:                     4316\n",
      "Model Family:        NegativeBinomial   Df Model:                           14\n",
      "Link Function:                    Log   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -22137.\n",
      "Date:                Thu, 12 Jun 2025   Deviance:                       980.59\n",
      "Time:                        11:31:06   Pearson chi2:                 1.26e+03\n",
      "No. Iterations:                    12   Pseudo R-squ. (CS):             0.4360\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================================\n",
      "                                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "Intercept                      4.1008      0.015    267.075      0.000       4.071       4.131\n",
      "Contaminacion_aire             0.1682      0.040      4.184      0.000       0.089       0.247\n",
      "I(Contaminacion_aire ** 2)    -0.0525      0.039     -1.356      0.175      -0.128       0.023\n",
      "Muertes_agua                  -0.6865      0.081     -8.492      0.000      -0.845      -0.528\n",
      "I(Muertes_agua ** 2)           1.4625      0.209      7.011      0.000       1.054       1.871\n",
      "I(Muertes_agua ** 3)          -0.8684      0.146     -5.932      0.000      -1.155      -0.581\n",
      "Exp_plomo                     -1.6529      0.102    -16.233      0.000      -1.852      -1.453\n",
      "I(Exp_plomo ** 2)              1.6201      0.211      7.689      0.000       1.207       2.033\n",
      "I(Exp_plomo ** 3)             -0.5203      0.128     -4.071      0.000      -0.771      -0.270\n",
      "Pesticidas                     0.4406      0.065      6.806      0.000       0.314       0.567\n",
      "I(Pesticidas ** 2)            -0.6277      0.139     -4.505      0.000      -0.901      -0.355\n",
      "I(Pesticidas ** 3)             0.2678      0.091      2.936      0.003       0.089       0.447\n",
      "Precipitaciones                0.6432      0.101      6.357      0.000       0.445       0.842\n",
      "I(Precipitaciones ** 2)       -1.6230      0.228     -7.133      0.000      -2.069      -1.177\n",
      "I(Precipitaciones ** 3)        0.9277      0.143      6.496      0.000       0.648       1.208\n",
      "==============================================================================================\n",
      "\n",
      "RMSE: 44.8368\n",
      "MAE: 29.6271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luism\\anaconda3\\envs\\TFG\\lib\\site-packages\\statsmodels\\genmod\\families\\family.py:1367: ValueWarning: Negative binomial dispersion parameter alpha not set. Using default value alpha=1.0.\n",
      "  warnings.warn(\"Negative binomial dispersion parameter alpha not \"\n"
     ]
    }
   ],
   "source": [
    "df = cargar_csv(\"Tabla_modelos.csv\")\n",
    "variables_independientes = ['Contaminacion_aire', 'Muertes_agua', 'Exp_plomo', 'Pesticidas','Precipitaciones']\n",
    "variable_dependiente = 'Parkinson'\n",
    "\n",
    "modelo_qp, rmse_qp, mae_qp = entrenar_quasi_poisson(df,variables_independientes, variable_dependiente, scaler=True)\n",
    "\n",
    "\n",
    "df = cargar_csv(\"Tabla_modelos.csv\")\n",
    "modelo_nb, rmse_nb, mae_nb = entrenar_binomial_negativo(df,variables_independientes, variable_dependiente,  scaler=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642c39fc-0c3c-4f1f-9e61-1fe33a6bc803",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62113dde-792e-4649-8759-82f5bd7c3f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 8.398972097919632\n",
      "RMSE: 18.031570172558176\n",
      "R²: 0.9500314014415512\n",
      "\n",
      "Importancia de las variables:\n",
      "             Variable  Importancia\n",
      "2           Exp_plomo     0.586057\n",
      "3          Pesticidas     0.140801\n",
      "1        Muertes_agua     0.129567\n",
      "4     Precipitaciones     0.099975\n",
      "0  Contaminacion_aire     0.043599\n",
      "             Variable  Importancia Media  Desviación\n",
      "2           Exp_plomo           0.653344    0.024747\n",
      "1        Muertes_agua           0.365144    0.016616\n",
      "3          Pesticidas           0.328642    0.019597\n",
      "4     Precipitaciones           0.219328    0.018890\n",
      "0  Contaminacion_aire           0.063649    0.007532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd  # Asegúrate de importar pandas\n",
    "\n",
    "# Leer los datos\n",
    "df = pd.read_csv(\"Tabla_modelos.csv\")\n",
    "\n",
    "# Suponiendo que tienes un DataFrame llamado df con las columnas y variables necesarias\n",
    "X = df[['Contaminacion_aire', 'Muertes_agua','Exp_plomo','Pesticidas', 'Precipitaciones']]\n",
    "y = df['Parkinson']\n",
    "\n",
    "# Dividir en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el modelo Random Forest con hiperparámetros ajustados\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=1000,        # Número de árboles\n",
    "    max_depth=None,             # Profundidad máxima de los árboles\n",
    "    min_samples_split=2,      # Mínimo de muestras para dividir un nodo\n",
    "    min_samples_leaf=1,       # Mínimo de muestras en una hoja\n",
    "    max_features=None,      # Usar la raíz cuadrada del número de características\n",
    "    random_state=42           # Fijar la semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Ajustar el modelo\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Imprimir resultados\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R²: {r2}\")\n",
    "\n",
    "# Obtener importancia de características\n",
    "importancia_variables = rf_model.feature_importances_\n",
    "\n",
    "# Crear un DataFrame con los nombres de las variables y su importancia\n",
    "importancia_df = pd.DataFrame({\n",
    "    'Variable': X.columns,\n",
    "    'Importancia': importancia_variables\n",
    "})\n",
    "\n",
    "# Ordenar por importancia descendente\n",
    "importancia_df = importancia_df.sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "# Mostrar la tabla de importancia\n",
    "print(\"\\nImportancia de las variables:\")\n",
    "print(importancia_df)\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Ejecutar permutación\n",
    "resultado = permutation_importance(rf_model, X_test, y_test, n_repeats=30, random_state=42)\n",
    "\n",
    "# Mostrar resultados\n",
    "importancia_perm = pd.DataFrame({\n",
    "    'Variable': X.columns,\n",
    "    'Importancia Media': resultado.importances_mean,\n",
    "    'Desviación': resultado.importances_std\n",
    "}).sort_values(by='Importancia Media', ascending=False)\n",
    "\n",
    "print(importancia_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21c45fc8-80bb-42ca-9dc9-0ca84d8446a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Mejores hiperparámetros encontrados:\n",
      "{'max_depth': 50, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 484}\n",
      "\n",
      "Resultados con mejores hiperparámetros:\n",
      "MAE: 8.3972\n",
      "RMSE: 17.9856\n",
      "R²: 0.9503\n",
      "\n",
      "Importancia de las variables:\n",
      "             Variable  Importancia\n",
      "2           Exp_plomo     0.585905\n",
      "3          Pesticidas     0.140894\n",
      "1        Muertes_agua     0.129022\n",
      "4     Precipitaciones     0.100471\n",
      "0  Contaminacion_aire     0.043708\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import randint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Definir variables predictoras y objetivo\n",
    "X = df[['Contaminacion_aire', 'Muertes_agua', 'Exp_plomo', 'Pesticidas', 'Precipitaciones']]\n",
    "y = df['Parkinson']\n",
    "\n",
    "# Dividir datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': randint(2, 11),\n",
    "    'min_samples_leaf': randint(1, 5),\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Configurar búsqueda aleatoria\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,  # número de combinaciones aleatorias\n",
    "    cv=5,  # validación cruzada\n",
    "    scoring='neg_mean_squared_error',  # también puedes usar 'r2'\n",
    "    n_jobs=-1,  # usar todos los núcleos disponibles\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Ejecutar búsqueda\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Mejor modelo encontrado\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Evaluar modelo con test\n",
    "mejor_modelo = random_search.best_estimator_\n",
    "y_pred = mejor_modelo.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nResultados con mejores hiperparámetros:\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "# Importancia de variables\n",
    "importancia_df = pd.DataFrame({\n",
    "    'Variable': X.columns,\n",
    "    'Importancia': mejor_modelo.feature_importances_\n",
    "}).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "print(\"\\nImportancia de las variables:\")\n",
    "print(importancia_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f741d5-cd05-4567-866c-3255c0551dd8",
   "metadata": {},
   "source": [
    "## XGBoosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b6e3efb-3c5f-4d23-9e8b-43ba04df7b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Mejores hiperparámetros encontrados para XGBoost:\n",
      "{'colsample_bytree': np.float64(0.9541329429833268), 'gamma': np.float64(1.197809453334862), 'learning_rate': np.float64(0.05346846162736693), 'max_depth': 6, 'n_estimators': 829, 'reg_alpha': np.float64(0.9856504541106007), 'reg_lambda': np.float64(0.2420552715115004), 'subsample': np.float64(0.8360677737029393)}\n",
      "\n",
      "Resultados con mejores hiperparámetros (XGB):\n",
      "MAE: 8.3559\n",
      "RMSE: 14.9229\n",
      "R²: 0.9658\n",
      "\n",
      "Importancia de las variables (XGB):\n",
      "             Variable  Importancia\n",
      "2           Exp_plomo     0.563107\n",
      "3          Pesticidas     0.148229\n",
      "1        Muertes_agua     0.128437\n",
      "4     Precipitaciones     0.113373\n",
      "0  Contaminacion_aire     0.046854\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import randint, uniform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Leer datos\n",
    "df = pd.read_csv(\"Tabla_modelos.csv\")\n",
    "X = df[['Contaminacion_aire', 'Muertes_agua', 'Exp_plomo', 'Pesticidas', 'Precipitaciones']]\n",
    "y = df['Parkinson']\n",
    "\n",
    "# Dividir conjunto\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir espacio de búsqueda\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'subsample': uniform(0.5, 0.5),\n",
    "    'colsample_bytree': uniform(0.5, 0.5),\n",
    "    'gamma': uniform(0, 5),\n",
    "    'reg_alpha': uniform(0, 1),\n",
    "    'reg_lambda': uniform(0, 1)\n",
    "}\n",
    "\n",
    "# Configurar RandomizedSearchCV\n",
    "xgb_random_search = RandomizedSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42, objective='reg:squarederror'),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Ejecutar búsqueda\n",
    "xgb_random_search.fit(X_train, y_train)\n",
    "\n",
    "# Mostrar los mejores hiperparámetros\n",
    "print(\"Mejores hiperparámetros encontrados para XGBoost:\")\n",
    "print(xgb_random_search.best_params_)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "mejor_xgb = xgb_random_search.best_estimator_\n",
    "y_pred = mejor_xgb.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nResultados con mejores hiperparámetros (XGB):\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "# Importancia de variables\n",
    "importancia_df = pd.DataFrame({\n",
    "    'Variable': X.columns,\n",
    "    'Importancia': mejor_xgb.feature_importances_\n",
    "}).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "print(\"\\nImportancia de las variables (XGB):\")\n",
    "print(importancia_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bef2ec9-6dd6-4b94-8baa-d785751eab15",
   "metadata": {},
   "source": [
    "## SVR Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88f68b96-b9e7-4782-87bf-12d0ff05abf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- VARIABLES ORIGINALES + HIPERPARÁMETROS AMPLIOS ---\n",
      "Mejores hiperparámetros: {'C': 1000, 'epsilon': 1, 'gamma': 10, 'kernel': 'rbf'}\n",
      "MAE: 13.47\n",
      "RMSE: 24.37\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos\n",
    "df = pd.read_csv(\"Tabla_modelos.csv\")\n",
    "\n",
    "# Variables originales\n",
    "variables_independientes = ['Contaminacion_aire', 'Muertes_agua', 'Exp_plomo', 'Pesticidas', 'Precipitaciones']\n",
    "X = df[variables_independientes]\n",
    "y = df['Parkinson']\n",
    "\n",
    "# División y escalado\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Modelo\n",
    "svr = SVR()\n",
    "\n",
    "# Rango más amplio de hiperparámetros\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'epsilon': [0.001, 0.01, 0.05, 0.1, 0.5, 1],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(estimator=svr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluación\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"--- VARIABLES ORIGINALES + HIPERPARÁMETROS AMPLIOS ---\")\n",
    "print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
    "print(\"MAE:\", round(mae, 2))\n",
    "print(\"RMSE:\", round(rmse, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86396ce0-67df-4185-a479-caa9c56a34f0",
   "metadata": {},
   "source": [
    "## KNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1b09855-0dc5-4549-afbb-f1b568518c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros encontrados: {'algorithm': 'auto', 'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'distance'}\n",
      "MAE KNN con variables originales: 11.77\n",
      "RMSE KNN con variables originales: 22.55\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Cargar los datos\n",
    "df = pd.read_csv(\"Tabla_modelos.csv\")\n",
    "\n",
    "# Definir las características (X) y la variable dependiente (y) con las variables originales\n",
    "variables_independientes = ['Contaminacion_aire', 'Muertes_agua', 'Exp_plomo', 'Pesticidas', 'Precipitaciones']\n",
    "X = df[variables_independientes]\n",
    "y = df['Parkinson']\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Estandarizar las características (importante para modelos como KNN)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Crear el modelo KNN\n",
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range(3, 51, 2)),  # Incrementar el número de vecinos hasta 50\n",
    "    'weights': ['uniform', 'distance'],  # Eliminar 'kernel', solo 'uniform' o 'distance'\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  # Mantener todos los algoritmos posibles\n",
    "    'metric': ['minkowski', 'euclidean', 'manhattan', 'chebyshev']  # Eliminar 'cosine', dejar métricas válidas\n",
    "}\n",
    "# Configurar GridSearchCV para encontrar la mejor combinación de parámetros\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Ajustar el modelo con el conjunto de entrenamiento\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Mostrar los mejores parámetros encontrados\n",
    "print(f\"Mejores parámetros encontrados: {grid_search.best_params_}\")\n",
    "\n",
    "# Predecir en el conjunto de prueba utilizando el mejor modelo encontrado\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_knn = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluar el modelo\n",
    "mae_knn = mean_absolute_error(y_test, y_pred_knn)\n",
    "rmse_knn = np.sqrt(mean_squared_error(y_test, y_pred_knn))\n",
    "\n",
    "# Imprimir los resultados de evaluación\n",
    "print(f\"MAE KNN con variables originales: {mae_knn:.2f}\")\n",
    "print(f\"RMSE KNN con variables originales: {rmse_knn:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c420d12-58c1-434e-b52a-fc4fce399788",
   "metadata": {},
   "source": [
    "## MLP Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd41d5d6-7edf-45b1-9db1-d90f98c77071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros encontrados: {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (128, 64), 'learning_rate': 'constant', 'max_iter': 1000, 'solver': 'adam'}\n",
      "Pérdida en el conjunto de prueba (MSE): 1035.4964948694842\n",
      "Primeras 5 predicciones: [ 22.08098554 150.27995381  32.18357354 280.17076001  90.77511977]\n",
      "Primeros 5 valores reales: [ 25.442265 115.928024  18.397535 264.6368   134.62796 ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Paso 1: Cargar los datos\n",
    "df = pd.read_csv(\"Tabla_modelos.csv\")\n",
    "\n",
    "# Paso 2: Seleccionar las características (X) y la variable dependiente (y)\n",
    "X = df[['Contaminacion_aire', 'Muertes_agua', 'Exp_plomo', 'Pesticidas', 'Precipitaciones']]\n",
    "y = df['Parkinson']  # Variable a predecir\n",
    "\n",
    "# Paso 3: Estandarizar las características (importante para redes neuronales)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Paso 4: Dividir los datos en entrenamiento y prueba (80% entrenamiento, 20% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paso 5: Crear el modelo de regresión (MLPRegressor)\n",
    "model = MLPRegressor(random_state=42)\n",
    "\n",
    "# Paso 6: Definir el rango de parámetros a buscar\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,), (128, 64)],  # Opciones más reducidas\n",
    "    'activation': ['relu'],  # Usar solo relu al principio\n",
    "    'solver': ['adam'],  # Usar solo adam\n",
    "    'max_iter': [500, 1000],  # Reducir el número de iteraciones\n",
    "    'alpha': [0.0001, 0.001],  # Regularización más restringida\n",
    "    'learning_rate': ['constant']  # Solo constante\n",
    "}\n",
    "# Paso 7: Configurar GridSearchCV para buscar los mejores parámetros\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Paso 8: Entrenar el modelo con la búsqueda de parámetros\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Paso 9: Mostrar los mejores parámetros encontrados\n",
    "print(f\"Mejores parámetros encontrados: {grid_search.best_params_}\")\n",
    "\n",
    "# Paso 10: Evaluar el modelo con los mejores parámetros encontrados\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Pérdida en el conjunto de prueba (MSE): {mse}\")\n",
    "\n",
    "# Paso 11: Hacer predicciones\n",
    "print(f\"Primeras 5 predicciones: {y_pred[:5]}\")\n",
    "print(f\"Primeros 5 valores reales: {y_test[:5].values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bde83d-e2a8-4a67-b64a-0fbaa1bc2d57",
   "metadata": {},
   "source": [
    "### Ajuste manual apartir de los mejores hiperparámetros obtenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c725d133-fb53-4c35-80de-fc5497984d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pérdida en el conjunto de prueba (MSE): 798.8951063623582\n",
      "Primeras 5 predicciones: [ 28.20950673 145.4198799   17.01147226 266.07157469  88.42056943]\n",
      "Primeros 5 valores reales: [ 25.442265 115.928024  18.397535 264.6368   134.62796 ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Paso 1: Cargar los datos\n",
    "df = pd.read_csv(\"Tabla_modelos.csv\")\n",
    "\n",
    "# Definir las características (X) y la variable dependiente (y) con las variables originales\n",
    "variables_independientes = ['Contaminacion_aire', 'Muertes_agua', 'Exp_plomo', 'Pesticidas', 'Precipitaciones']\n",
    "X = df[variables_independientes]\n",
    "y = df['Parkinson']\n",
    "\n",
    "\n",
    "\n",
    "# Paso 3: Estandarizar las características (importante para redes neuronales)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Paso 4: Dividir los datos en entrenamiento y prueba (80% entrenamiento, 20% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paso 5: Crear el modelo de regresión (MLPRegressor)\n",
    "model = MLPRegressor(hidden_layer_sizes=(256, 128), activation='relu', max_iter=5000,alpha=0.01, random_state=42)\n",
    "\n",
    "# Paso 6: Entrenar el modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Paso 7: Evaluar el modelo\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Pérdida en el conjunto de prueba (MSE): {mse}\")\n",
    "\n",
    "# Paso 8: Hacer predicciones\n",
    "print(f\"Primeras 5 predicciones: {y_pred[:5]}\")\n",
    "print(f\"Primeros 5 valores reales: {y_test[:5].values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b099f8f-fffa-441f-a6b2-801f4656ae00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
